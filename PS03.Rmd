---
title: "STAT/MATH 495: Problem Set 03"
author: "Brendan Seto"
date: "2017-09-26"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4.5)

# Load packages
library(tidyverse)
library(mosaic)

data1 <- read_csv("data/data1.csv")
data2 <- read_csv("data/data2.csv")
```


# Question

For both `data1` and `data2` tibbles (a tibble is a data frame with some
[metadata](https://blog.rstudio.com/2016/03/24/tibble-1-0-0#tibbles-vs-data-frames) attached):

* Find the splines model with the best out-of-sample predictive ability.
* Create a visualizaztion arguing why you chose this particular model.
* Create a visualizaztion of this model plotted over the given $(x_i, y_i)$ points for $i=1,\ldots,n=3000$.
* Give your estimate $\widehat{\sigma}$ of $\sigma$ where the noise component $\epsilon_i$ is distributed with mean 0 and standard deviation $\sigma$.


# Data 1

## Finding Best DF
The first thing to do was determine the optimal degrees of freedom.  This was accomplished by creating a function that created spline models with incrementally increasing df, then graphing the resulting RMSE.  The df with the lowest RMSE is optimal.  


```{r, Function, warning=FALSE, message=FALSE}
# Seperate my training/testing data
train1 <- sample_n(data1, 1500)
test1 <- anti_join(data1, train1, by = "ID")

# Create spline models
bestdf <- function(df){
  ## Right
  m_splineR <- with(train1, smooth.spline(x, y, df=df))
  outputR <- predict(m_splineR, test1$x) %>% 
    tibble::as.tibble() %>% cbind(true = test1$y) %>% 
    # Score with RMSE
    mutate(se = (true-y)^2) %>% summarise(R = sqrt(mean(se)))

  ## Left
  m_splineL <- with(test1, smooth.spline(x, y, df=df))
  outputL <- predict(m_splineL, train1$x) %>% 
    tibble::as.tibble() %>% cbind(true = train1$y) %>% 
    # Score with RMSE
    mutate(se = (true-y)^2) %>% summarise(L = sqrt(mean(se)))
  output <- c(outputR$R, outputL$L, (outputR$R+outputL$L)/2,df)
  return(output)
}



all <- as.data.frame(t(sapply(seq(1.1,100,.1), bestdf)))
colnames(all) <- c("R","L","RMSE","df")
all <- all %>% gather(type,RMSE, R:RMSE)
```

Figure 1 represents the RMSE vs DF of my spline models.  Note that the train and test data had a 50-50 split.  Thus we expect that switching the labels of our two datasets (noted right and left in the code) should not change our results too much.  Interestingly there is a change.  I'll look at that with the next data.  

The black dashed line represents the minimum of the function.  The value and associated RMSE is presented below.  Note that **RMSE is the estimate of $\widehat{\sigma}$**!

```{r Optimal DF}
optimal <- all %>% filter(RMSE == min(all %>% filter(type == "RMSE") %>% select(RMSE)))
optimal
```

```{r Plot}
ggplot(all) + geom_point(aes(df, RMSE, color = type), size=1, alpha=0.1) +
  geom_point(data=filter(all, type=="RMSE"),aes(df, RMSE, color = type), size=1) +
  theme_bw()+
  labs(list(title = "Figure 1: RMSE vs DF. Spline Model of Data 1"))+
  geom_vline(xintercept = optimal$df, linetype = "longdash")

```


## Spline Model

```{r Spline Model}
m1 <-  with(train1, smooth.spline(x, y, df=optimal$df))
output <- predict(m1, test1$x) %>% 
    tibble::as.tibble()

ggplot(output) + geom_line(aes(x,y, color = "Best Spline Model"), size=2) + 
  geom_point(data=data1, aes(x,y), alpha=0.2)+
  labs(list(title = "Optimal Spline Model"))
```


# Data 2

Time for the second trial!  This time I calculated the optimal df and RMSE for 10 different samples.  After that, the procedure is the same.  

```{r, Function 2, warning=FALSE, message=FALSE}
# Create spline models
bestdf2 <- function(df, data){
  
  # Seperate my training/testing data
  train2 <- sample_n(data, 1500)
  test2 <- anti_join(data, train2, by = "ID")
  
  ## Right
  m_splineR <- with(train2, smooth.spline(x, y, df=df))
  outputR <- predict(m_splineR, test2$x) %>% 
    tibble::as.tibble() %>% cbind(true = test2$y) %>% 
    # Score with RMSE
    mutate(se = (true-y)^2) %>% summarise(R = sqrt(mean(se)))

  ## Left
  m_splineL <- with(test2, smooth.spline(x, y, df=df))
  outputL <- predict(m_splineL, train2$x) %>% 
    tibble::as.tibble() %>% cbind(true = train2$y) %>% 
    # Score with RMSE
    mutate(se = (true-y)^2) %>% summarise(L = sqrt(mean(se)))
  output <- c((outputR$R+outputL$L)/2,df)
  return(output)
}


many <- function(data){
  all2 <- as.data.frame(t(sapply(seq(1.1,100,.1), bestdf2,data=data)))
  colnames(all2) <- c("RMSE","df")
  optimal2 <- all2 %>% filter(RMSE == min(all2  %>% select(RMSE)))
  return(optimal2)
}

# manyTrials <- do(10, parallel = TRUE)*many(data2)
# Doesn't work in a reasonable amount of time.  I ran it, then saved the data.  Will just read in when kniting.  
# write.csv(manyTrials, "manyTrials.csv")

manyTrials <- read.csv("manyTrials.csv")

```

```{r, Optimal 2}
est <- manyTrials %>% summarise(dfSD = sd(df),df = mean(df), RMSEsd = sd(RMSE),  RMSEM = mean(RMSE))
est
```

Interesting.  The df does vary a lot.  In the (relatively small) sample, the sd was `r round(est$dfSD, 2)`.  This is in contrast to the RMSE, which had an sd of `r round(est$RMSEsd, 2)`.  This suggests we do not have to be extraordinarily precise in our df estimates, although the small sample size of this experiment does provide some caveats.    

Recall again that RMSE is the estimate of $\widehat{\sigma}$.  

## Spline Model

```{r, Spline 2}
m2 <-  with(data2, smooth.spline(x, y, df=est$df))
output <- predict(m2, data2$x) %>% 
    tibble::as.tibble()

ggplot(output) + geom_line(aes(x,y, color = "Best Spline Model"), size=2) + 
  geom_point(data=data2, aes(x,y),alpha=0.2)+
  labs(list(title = "Optimal Spline Model"))
```


# Try 3

I'm curious to know if the number of folds effects the final result.  I'll use data 1 for this.  

First thing I did is create a function that takes df and test data and outputs the resulting RMSE.  Note that I could go through the whole process of finding optimal df again.  I tried it, but the function took forever to run and really wasn't worth it.  I'm really just interested in how RMSE and df changes by number of folds and setting cv=TRUE on my spline models allows me to see that.  

```{r, Spline model 3,warning=FALSE}
output <- function(test){
  
  # Take test data out of train
  train <- anti_join(data1, test, by = "ID")
  
  ## Spline Model
  m_spline <- with(train, smooth.spline(x, y, cv = TRUE))
  output <- predict(m_spline, test$x) %>% 
    tibble::as.tibble() %>% cbind(true = test$y) %>% 
    # Score with RMSE
    mutate(se = (true-y)^2) %>% summarise(RMSE = sqrt(mean(se)), df = m_spline$df)
  
  # Return RMSE and df for this dataset
  return(output)
}
```

I can now pass different test folds into this the output function to get an average RMSE

```{r, Folds Function, warning=FALSE}
folds <- function(n){
  nr = nrow(data1)
  ## Split the dataframe into different ammounts of folds. Run output function on each
  ave <-   sapply(seq(1,n), function(i) output(split(data1, rep(1:ceiling(nr/n), each=n, length.out=nr))[[i]]))
  
  ## Return average RMSE accross all folds
  return(cbind(RMSE = mean(unlist(ave[1,])), df = ave[2,1]))
}
```

Now I run my function with 2 through 20 folds and see what happens.

```{r, warning=FALSE}
f <- as.data.frame(t(sapply(seq(2,20), folds))) %>% mutate(fold = seq(2,20))
colnames(f) <- c("df","RMSE", "fold")
f$df <- unlist(f$df)
```

Here is a plot of the results.  Neither the RMSE or df seem to vary greatly after the first couple folds.  Very early number of folds (2-4) may have lower optimal df, but the difference is not very stark.  

```{r}
ggplot(f) + geom_point(aes(fold,unlist(RMSE), size=df, color = df))+
  labs(title = "Effect of Number of Folds on RMSE and df for Spline Models", x = "Number of Folds", y="RMSE")
```

